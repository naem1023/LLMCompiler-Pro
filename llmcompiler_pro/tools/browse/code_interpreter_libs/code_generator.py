import json

from langchain_core.messages.base import BaseMessage
from langchain_core.messages.human import HumanMessage
from langchain_core.messages.system import SystemMessage
from langchain_openai import ChatOpenAI
from logzero import logger

from main.auto_agent.prompt_render.jinja2_render import Jinja2Render
from main.streaming_handlers.chainlit_code_interpreter_updater import (
    AutoAgentCodeInterpreterTracer,
)


class CodeGenerator:
    def __init__(self, model_name: str, callbacks: list = None, session_id: str = None):
        self.llm = ChatOpenAI(
            model_name=model_name,
            streaming=True,
            # callbacks=callbacks,
            temperature=0.2,
            model_kwargs={"response_format": {"type": "json_object"}},
        )
        self.prompt_render = Jinja2Render("demo/prompt")
        self.session_id = session_id

    def compose_chat_history(
        self,
        user_input: str,
        chat_history: list[BaseMessage],
        plan: str,
        current_goal: str,
    ) -> list[BaseMessage]:
        system_prompt = self.prompt_render.render(
            "auto_agent",
            "code_generator_system_prompt.jinja2",
        )

        user_prompt = self.prompt_render.render(
            "auto_agent",
            "code_generator_user_prompt.jinja2",
            user_input=user_input,
            whole_plan=plan,
            current_goal=current_goal,
        )

        chat_history = [
            SystemMessage(
                content=system_prompt,
            ),
            *chat_history,
            HumanMessage(
                content=user_prompt,
            ),
        ]

        return chat_history

    async def generate(
        self,
        user_input: str,
        chat_history: list[BaseMessage],
        plan: str,
        current_goal: str,
    ) -> list | str:
        """Generate thought and code snippet, or only the text.

        It doesn't consider the chat history summarization, personalization, and other factors.
        Planner should provide these information via chat history.

        Return:
            when return type is list:
                Code generation is success using json model.
                But it can't generate the list object directly, so generation is guided to generate like this.
                {
                    "response": [ ... ],
                }
                And finally the return value is the list in the "response" key.
            when return type is str:
                LLM failed to generate the valid json.
        """
        chat_history = self.compose_chat_history(
            user_input, chat_history, plan, current_goal
        )

        streaming_info = f"Instruction generated by plan: {current_goal}"
        callbacks = [AutoAgentCodeInterpreterTracer(streaming_info)]

        response = await self.llm.ainvoke(chat_history, config={"callbacks": callbacks})

        logger.info(f"response: {type(response)}, {response}")

        try:
            response = json.loads(response.content)
            if "response" in response:
                response = response.get("response")
        except json.JSONDecodeError:
            logger.error(
                f"Failed to decode the response in the CodeGenerator: {response.content}"
            )
            response = response

        return response
